import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense
# Sample text dataset (you can replace this with any large text data)
text = """Deep learning is a subset of machine learning that uses neural networks to learn from large datasets. 
          Recurrent Neural Networks are powerful for sequence-based tasks such as text generation and speech recognition."""

# Convert text to lowercase
text = text.lower()

# Initialize Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])  # Create vocabulary from the text

# Get total number of unique words
total_words = len(tokenizer.word_index) + 1

# Convert text into sequences of numbers
input_sequences = []
words = text.split()

for i in range(1, len(words)):
    n_gram_sequence = words[:i+1]  # Take words progressively
    seq = tokenizer.texts_to_sequences([" ".join(n_gram_sequence)])[0]  # Convert to numbers
    input_sequences.append(seq)

# Find maximum sequence length
max_seq_length = max(len(seq) for seq in input_sequences)

# Pad sequences to ensure uniform length
input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='pre')

# Split into X (features) and y (labels)
X, y = input_sequences[:, :-1], input_sequences[:, -1]

# Convert y to categorical (one-hot encoding)
y = tf.keras.utils.to_categorical(y, num_classes=total_words)
# Define the LSTM model
model = Sequential([
    Embedding(total_words, 10, input_length=max_seq_length - 1),  # Embedding Layer
    LSTM(100, return_sequences=True),  # First LSTM Layer
    LSTM(100),  # Second LSTM Layer
    Dense(100, activation='relu'),  # Fully Connected Hidden Layer
    Dense(total_words, activation='softmax')  # Output Layer with Softmax
])

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Display model summary
model.summary()
# Train the model
model.fit(X, y, epochs=200, verbose=1)
import random

def generate_text(seed_text, next_words=5):
    for _ in range(next_words):
        token_list = tokenizer.texts_to_sequences([seed_text])[0]  # Convert input to tokens
        token_list = pad_sequences([token_list], maxlen=max_seq_length - 1, padding='pre')  # Pad sequence
        predicted = np.argmax(model.predict(token_list, verbose=0), axis=-1)  # Predict next word
        
        # Find the word that corresponds to the predicted index
        for word, index in tokenizer.word_index.items():
            if index == predicted:
                seed_text += " " + word
                break
    return seed_text

# Example: Generate text
print(generate_text("deep learning", next_words=10))
print(generate_text("machine learning", next_words=10))
print(generate_text("neural networks", next_words=10))

